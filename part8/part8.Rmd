---
title: "Introduction to Tidymodels"
author: "Ted Laderas"
date: "12/7/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Before you get started

Please review the machine learning introduction before you start working with this notebook. It will acquaint you with the terminology you will need to understand to work with `tidymodels`.

# Glossary

-   *Machine Learning* - utilizing algorithms to discover and utilize patterns in a dataset
-   *Engine* - `tidymodels` speak for a machine learning package that has an algorithm - usually a specific package such as `ranger`
-   *Features* - machine learning speak for *variables* used in your model (usually a column) used to predict your outcome (also known as *predictors*)
-   *Training Set* - a set of data used for training your models
-   *Test Set* - a set of data used for testing your model - *must* be a distinct subset from the training set.
-   *Unsupervised Learning* - A machine learning task for examining groupings/variability in a dataset. Examples include clustering, principle components analysis, TSNE.
-   *Supervised Learning* - A machine learning task for predicting the identity of a sample (usually a row) based on other data.



# Caveat

This is meant to only be an introduction to the machine learning workflow rather than a comprehensive overview. I highly recommend that you think about taking an online machine learning course to follow this up.

There is a neural network/deep learning course at OHSU, and Xubo Song's Machine Learning course in DMICE, as well as Melanie Mitchell's machine learning course at PSU.



# Learning Objectives

-   *Utilize* the `resample` package to produce test/train datasets
-   *Understand* how the `recipes` package makes preprocessing reproducible
-   *Apply* a recipe to a dataset to preprocess it
-   *Utilize* data reduction methods, such as PCA for exploratory data analysis.
-   *Run* and *interpret* three different machine learning methods and compare them



# What is `tidymodels`?

There are a lot of different packages and machine learning methods available for R. One big issue is that the output of all of these models is not standardized - for example, if you wanted a p-value from a model, you'd look in different places for the results.

The `tidymodels` workflow is designed to map to common tasks you use for machine learning.



## The different parts of `tidymodels`

The different sections of `tidymodels` are designed to be useful in a `tidy` workflow and roughly map to the different steps and requirements of a machine learning workflow.



## Let's run through a basic `tidymodels` workflow

These are the major packages where `tidymodels` is used.

-   {resample} - use these functions to specify a test/training set, or to build a cross-validation set
-   {recipes} - use these functions to normalize variables
-   {parsnip} - use these functions to specify and train your model
-   {workflows} - use a model and recipe together (allows you to switch out models and use them)
-   {yardstick} - use these functions to evaluate your model (accuracy on test data)

More advanced packages:

-   {tune} - helps you decide the optimal parameters for a model by running models at different levels of that parameter
-   {stacks} - lets you run model ensembles of ML algorithms



## Step 1: Explore the Data First

We need to do a little bit of tidying on the `penguins` data before we can use it. The first thing we need to do is select the numeric measurements, and then subset the species to be only two species instead of three.

```{r}
library(palmerpenguins)
library(tidymodels)
library(rsample)
library(tidyverse)

data("penguins")

penguins_matrix <- penguins %>% 
  select(species, c(contains("mm"), contains("_g"))) %>%
  filter(species %in% c("Chinstrap", "Adelie")) %>%
  filter(complete.cases(.)) %>%
  mutate(species = forcats::fct_drop(species))
```

```{r}
skimr::skim(penguins_matrix)
```




# Step 2: {rsample} - split the data up

Build test/train set

```{r}
penguins_split <- initial_split(penguins_matrix, prop = 0.85, strata = species)

penguins_train <- training(penguins_split)
penguins_test <- rsample::testing(penguins_split)

dim(penguins_train)
```




# Step 3: {recipes} - process the data for use by our models

https://www.tidymodels.org/start/recipes/

- Specifying Features, outcomes, and id columns with model formula 
- Convert factors to dummy variables 
- Normalizing Numeric Data 
- Exploring Data with Principal Components/UMAP



### Starting a recipe with `recipe()`

All preprocessing begins with the `recipe()` function:


```
my_recipe <- recipe(species ~ . , data = penguin_train)
```

You can see that we provided an argument `data` to specify that our data should come from the `penguin_train` split from our data.



## How does recipes know what column is what?

There are three main column types in {recipes}:

- Outcomes (what we want to predict)
- Features (the columns we use to build our model)
- Identifiers (columns not used in analysis, but are useful as identifiers, such as sample ids)


We can specify the outcomes and features in `recipe()` using a formula:

```
species ~ . 
```

In our case, our outcome is `species` (because it is on the left of the `~`), and we use a `.` to denote that every other column is a potential feature, or predictor.



What if we had an id column in the data, such as `penguin_name`? We can use `update_role()` to tell `recipes` they're an id.


So, if we had `penguin_name` as an ID column in our data set, we'd do this at the beginning of our recipe:


```
my_recipe <- recipe(species ~ . , data = penguin_train) %>%
     update_role(penguin_name, role = "ID")
```



## A recipe consists of `step_`s

Now we have our basic recipe built, it's time to add processing for the columns. These processing steps all begin with `step_` such as:

- `step_normalize()` - scale all predictors to have a mean of 0 and standard deviation (SD) of 1
- `step_dummy()` - transform `factor` variables into dummy variables
- `step_arrange()` - sort data using `arrange()`


How does each step know what columns to process? We need to provide the column names as an argument. For example, if we wanted to just normalize the `bill_length_mm` and `bill_depth_mm` columns:


```
my_recipe <- recipe(species ~ . , data = penguin_train) %>%
   step_normalize(bill_length_mm, bill_depth_mm)
```



## Some handy selectors: `all_predictors()`

Using column names 

Some steps you may want to run on all the predictors in the dataset, so you instead of the column names, you can use *selectors* instead. 


If we wanted to `step_normalize` all the predictors, we can use `all_predictors()` instead of the column names:


```
my_recipe <- recipe(species ~ . , data = penguin_train) %>%
   step_normalize(all_predictors())

```

There are some built in `all_`* functions:

- `all_numeric()` - apply step to all numeric variables in the dataset
- `all_nominal()` - apply step to all categorical variables

In fact, if you know tidy select helpers, you can use these as well.




## Step 3a Building a Recipe: Principal Components Analysis

Let's build a recipe that helps us explore the data.

We might want to start understanding how difficult it is to actually predict the species.

One technique we can use is `Principal Components Analysis` PCA

```{r}
pca_rec <- recipe(species ~., data = penguins_train) %>%
  step_normalize(all_predictors()) %>%
  step_pca(all_predictors())

pca_prep <- prep(pca_rec)
pca_prep

tidied_pca <- tidy(pca_prep, 2)
```

##

If you want to see the data that has been processed by `recipes` you need to run the `juice` command to produce the transformed data.

```{r}
juice(pca_prep)
```

```{r}
juice(pca_prep) %>%
  ggplot(aes(PC1, PC2)) +
  geom_point(aes(color = species), alpha = 0.7, size = 2) +
  labs(color = NULL)
```

Plot `PC1` versuse `PC3`

```{r}
juice(pca_prep) %>%
  ggplot(aes(PC1, PC3)) +
  geom_point(aes(color = species), alpha = 0.7, size = 2) +
  labs(color = NULL)
```

### PCA: Contributions from each variable

Now we have our `tidied_pca` version of the data, let's take a look at the linear combinations 

```{r}
tidied_pca %>%
  filter(component %in% paste0("PC", 1:4)) %>%
  mutate(component = fct_inorder(component)) %>%
  ggplot(aes(value, terms, fill = terms)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~component, nrow = 1) +
  labs(y = NULL)
```




## Step 3b: Predicting penguin species - Setup a `recipe` for our ML model

Ok, the PCA gave us an idea of how hard it is to separate the two different penguin species.

We can set up a recipe for preprocessing the numeric data in our model. We use `step_normalize()` to standardize the ranges of each variable.

Again, we need to provide a formula at this step because it helps `tidymodels` identify what the outcome is, what the features are, and what shouldn't be included.

```{r}
classification_rec <- recipe(species ~., data = penguins_train) %>%
  step_normalize(all_predictors())

classification_rec
```

Let's take a look at the transformed data.

```{r}
class_prep <- prep(classification_rec)

juice(class_prep)

skimr::skim(juice(class_prep))
```


## Step 4 {parsnip} - specify the model

Once you have the preprocessed data that is split properly, you can use it as an input to {parsnip}. But first we need a model!

A model specifies the type of algorithm we want to use, and what *engine* we want to use to calculate it, and based on how it is used, the *mode* (usually classification or regression) we want to use it in.



- Specifying model type/engine 
- (Learning optimum parameters for a model {tune/rsample})
- Fitting the model to training data



### What models exist?

Nearly all of the popular machine learning methods work within the `tidymodels` framework.

<https://www.tidymodels.org/find/parsnip/>


### Step 4a - K Nearest Neighbor

A really robust model we can use to predict our `species` is `K-Nearest Neighbor`. 

```{r}
knn_model <- nearest_neighbor() %>% 
  set_engine("kknn") %>% 
  set_mode("classification")
```




## Step 5 {workflows}: fitting a recipe and a model together

We now have our recipe and our model. How do we tie them together?

We can use a *workflow* from the {workflows}

```{r}
knn_workflow <- workflow() %>%
  add_recipe(classification_rec) %>%
  add_model(knn_model)

knn_fit <- fit(knn_workflow, data = penguins_train)

knn_fit
```



## Step 6: {yardstick}: how well did we do on the test data?



```{r}
knn_fit <- knn_workflow %>%
  # fit on the training set and evaluate on test set
  last_fit(split = penguins_split)

test_performance <- knn_fit %>% collect_metrics()
test_performance
```



## Your Turn

Apply a decision tree using the following model and `classification_rec()` in a workflow. Compare how you did with KNN on the test data using `last_fit()`:


```{r}
tree_model <- decision_tree() %>%
  set_engine("rpart") %>%
  set_mode("classification")

tree_workflow <- workflow() %>%
  ------------- %>%
  -------------
  
tree_fit <- tree_workflow %>%
  last_fit(split = -------)



```


## Optional Steps (using `tune`)

We've gone through the basic workflow


### Random Forests

Here we're using the random forest model

```{r}
tree_model <- decision_tree() %>% 
  set_engine("rpart") %>% 
  set_mode("classification") 

tree_workflow <- workflow() %>%
  add_recipe(classification_rec) %>%
  add_model(tree_model)

tree_fit <- fit(tree_workflow, data = penguins_train)
```

```{r}
library(tune)

rf_model <- 
  # specify that the model is a random forest
  rand_forest() %>%
  # specify that the `mtry` parameter needs to be tuned
  set_args(mtry = tune()) %>%
  # select the engine/package that underlies the model
  set_engine("ranger", importance = "impurity") %>%
  # choose either the continuous regression or binary classification mode
  set_mode("classification") 

library(workflows)

rf_workflow <- workflow() %>%
  # add the recipe
  add_recipe(classification_rec) %>%
  # add the model
  add_model(rf_model)
```

### Tuning Results

We have a parameter, `mtry`, that we don't know the best value for. We can use `tune_grid()`

```{r}
penguins_cv <- rsample::vfold_cv(penguins_train)

rf_grid <- expand.grid(mtry = c(3, 4, 5))
# extract results
rf_tune_results <- rf_workflow %>%
  tune_grid(resamples = penguins_cv, #CV object
            grid = rf_grid, # grid of values to try
            metrics = metric_set(accuracy, roc_auc) # metrics we care about
            )

```

```{r}
rf_tune_results %>%
  collect_metrics()
```

```{r}
param_final <- rf_tune_results %>%
  select_best(metric = "accuracy")
param_final

```

```{r}
rf_workflow <- rf_workflow %>%
  finalize_workflow(param_final)
```

```{r}
rf_fit <- rf_workflow %>%
  # fit on the training set and evaluate on test set
  last_fit(penguins_split)
```

## Yardstick

Evaluating Accuracy Comparing different methods

```{r}
test_performance <- rf_fit %>% collect_metrics()
test_performance
```

```{r}
test_predictions <- rf_fit %>% collect_predictions()
test_predictions

test_predictions %>% 
  conf_mat(truth = species, estimate = .pred_class)

test_predictions %>%
  ggplot() +
  geom_density(aes(x = .pred_Adelie, fill = species), 
               alpha = 0.5)
```




## More Steps in Data Exploration

### Running UMAP as a recipe (Optional)

```{r}
library(embed)

umap_rec <- recipe(species ~., data = penguins_train) %>%
  step_normalize(all_predictors()) %>%
  step_umap(all_predictors())

umap_prep <- prep(umap_rec)

umap_prep

```

```{r}
umap_pca <- tidy(umap_prep, 2)

juice(umap_prep)

juice(umap_prep) %>%
  ggplot(aes(umap_1, umap_2)) +
  geom_point(aes(color = species), alpha = 0.7, size = 2) +
  labs(color = NULL)
```




## Assignment

1. Pick two recipe steps from this list and describe what they do and why you would use them in your own words:

- `step_corr()`
- `step_center()`
- `step_count()`
- `step_BoxCox()`

2. Modify `classification_rec`


2. Pick two ML methods to compare on `penguins`.

3. 


## Acknowledgements

Adapted from <http://www.rebeccabarter.com/blog/2020-03-25_machine_learning/> <http://www.rebeccabarter.com/blog/2019-06-06_pre_processing/> <https://juliasilge.com/blog/cocktail-recipes-umap/> <https://bcullen.rbind.io/post/2020-06-02-tidymodels-decision-tree-learning-in-r/>
