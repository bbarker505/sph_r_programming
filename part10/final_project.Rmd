---
title: "Final Project"
author: "You"
date: "3/2/2021"
output: 
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(tidymodels)
set.seed(100)
```

## Modeling NHANES

We will be working from an extract of a survey called NHANES. 

```{r}
library(NHANES)
data(NHANES)

NHANES <- janitor::clean_names(NHANES)

skimr::skim(NHANES)
```


## Picking an Outcome

> Pick one of the following outcomes to model: 

- diabetes
- depression
- sleep_trouble

> For non-binary outcomes, binarize them into two distinct groups by combining categories, such as

- None: None
- Depressed: (Several, Most)

> It may be easier to make this into a dummy variable, as it helps with the analysis and you can control how it's coded.


## Summarizing Data

> Pick the covariates you are interested in using as predictors. You can access a data dictionary of variables using `help(NHANES)`.

> Why are you interested in your predictors? What is your expectation?


## Discuss Models and covariates with your group

> Make sure that everyone is running different models.

> I will run and post the results for the following base models. You can use these for comparison:

- Diabetes: diabetes ~ bmi
- Depressed: depressed ~ poverty
- Sleep Trouble: sleep_trouble ~ age

## What are your two models?

> Specify your two models below.





## Logistic regression modeling of your data

Use the below `select()` statement to select your outcome and your covariates. Binarize your outcome to two categories.

```{r}
data(NHANES)

NHANES <- NHANES %>%
  janitor::clean_names() %>%
  #collapse outcome to be binary
  mutate(depressed = fct_collapse(depressed, depressed=c("Several", "Most"))) %>%  
  select(depressed, poverty, bmi, alcohol_day, age_first_marij, 
         sleep_hrs_night, hh_income_mid, little_interest)
```

## Splitting the data

> Split your data into test/train sets. 

```{r}
##Split the Data
NHANES_split <- rsample::initial_split(data = NHANES, strata = depressed, prop = 0.8)
NHANES_train <- training(NHANES_split)
NHANES_test <- testing(NHANES_split)
```

## Preprocess your data

> Run your recipe, and assess the impact of the preprocessing on the data. If you choose to use `step_naomit()`, then note the number of rows before preprocessing and after preprocessing. If you choose to use `step_dummy()`, note the names of the dummy variables generated.

```{r}
## Build the recipe
classification_recipe <- recipe(depressed ~ ., data=NHANES_train) %>%
  step_normalize(all_numeric()) %>%
  step_naomit(depressed, all_numeric()) %>%
  #make our one categorical variable into a dummy variable
  step_dummy(little_interest)

## apply the recipe with prep/bake
lr_prep <- prep(classification_recipe)
class_data_train <- bake(lr_prep, new_data=NULL)
```

## Specify and fit your model

> Fit both of your models here. 

```{r}
## Specify the logistic regression model
lr_model <- parsnip::logistic_reg() %>%
  set_engine("glm") %>%
  set_mode("classification")

## Fit the data to the training set
## if you used step_dummy(), note the names of the dummy variables
lr_fit <- fit(lr_model, depressed ~ poverty + little_interest_Several + little_interest_Most, data=class_data_train)

##Fit your second model here (maybe call it lr_fit2)
```

## Visualize the predicted probability distribution for your data

> Use `predict()` to visualize the predicted probability distributions for your outcome for both of your models. 

> If you want to choose a probability threshold, use `case_when()` in a `mutate()` statement to predict the classes. 

```{r}
class_data_test <- bake(lr_prep, new_data = NHANES_test)

prob_results <- lr_fit %>% 
  predict(new_data = class_data_test, type = "prob") %>%
  bind_cols(class_data_test) 

prob_results %>% 
  ggplot() + 
  aes(x= .pred_depressed, fill=depressed) +
  geom_histogram()
```


## Use predict to generate predictions on the test set

> If you chose your own threshold here, make sure to name your prediction column "estimate" and ensure it is a factor and has the same levels and order as your outcome.

> Generate predictions for both your models.

```{r}
# process the testdata using bake

# predict on the test set
test_results <- lr_fit %>% 
  predict(new_data = class_data_test, type = "class") %>%
  bind_cols(class_data_test) %>%
  rename(truth = depressed, estimate=.pred_class)

# predict with your second model here
```


## Report for your models

> For your two models, report balanced accuracy, sensitivity, and specificity of your model.

> You can make two lines in the spreadsheet for each of your models. 

Link is here: 

```{r}
#look at our confusion matrix
test_results %>%
  yardstick::conf_mat(truth = truth, estimate= estimate)

#look at our accuracy of our predicted classes
test_results %>%
  yardstick::bal_accuracy(truth = truth, estimate= estimate)

#look at our accuracy of our predicted classes
test_results %>%
  yardstick::sensitivity(truth = truth, estimate= estimate)

test_results %>%
  yardstick::specificity(truth = truth, estimate=estimate)
```


## Write up of your predictors

> For your two models, talk about the predictive power of your predictor variables. Is one predictor variable more predictive of your outcome than the others? 

> Are the results in line with your expecations? Would you say your model is a good predictor or not?

> As a group, write a couple paragraphs about all of the predictors and compare their predictive power to the base model. Which of the predictors was the most predictive for each outcome? You can do this as a Google doc, and then cut and paste the paragraph into your final report. 



## Optional: Use K-nearest neighbors to predict class

> For one of your models, try running k-nearest-neighbor on your data with the same model formula. Try different values of k and pick the best performing one.

> Assess the sensitivity, specificity, and balanced accuracy of your data. Add it as a separate line in the database.

```{r}
knn_model <- nearest_neighbor(neighbors = 7) %>% 
  set_engine("kknn") %>% 
  set_mode("classification")

knn_fit <- fit(object= knn_model, 
               formula = depressed ~ poverty + little_interest_Several + little_interest_Most,
               data = class_data_train)
```


```{r}
knn_predictions <- knn_fit %>%
  predict(new_data = class_data_test) %>%
  bind_cols(class_data_test) %>%
  rename(truth = depressed, estimate=.pred_class)

knn_predictions %>%
  janitor::tabyl(truth, estimate)

knn_predictions %>%
  yardstick::bal_accuracy(truth, estimate)

knn_predictions %>%
  yardstick::sensitivity(truth = truth, estimate= estimate)

knn_predictions %>%
  yardstick::specificity(truth = truth, estimate=estimate)
```

